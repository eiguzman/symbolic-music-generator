{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc56645-3d7f-4bac-a15a-1fbe24acefe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Symbolic Music Generation with Variational Autoencoders\n",
    "### CSE 153 - Assignment 2\n",
    "#### Edgar Guzman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9efce7-ca2d-4012-a92d-c0398cf66195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355a1dc-75c8-4c52-a3b0-4162da92516b",
   "metadata": {},
   "source": [
    "**Task**: *Make some beautiful music!*\n",
    "\n",
    "This project develops deep learning frameworks to generate symbolic music in both conditioned and unconditioned modes. Trained on thousands of copyright-free MIDI files, the goal is to develop structurally sound and audibly appealing piano compositions. The datasets used in this project are available as zip files from [PDMX](https://pnlong.github.io/PDMX.demo/), [MAESTRO](https://magenta.tensorflow.org/datasets/maestro#v300), and [MIDI Chords](https://github.com/ldrolez/free-midi-chords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e84e4-00fe-446b-a598-cc1d8c3d6500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Task 1a - 1D Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8b0f7-1e18-4ccf-95d8-1a49b001d72f",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "Our first model aims to generate music using symbolic, unconditioned generation. We train a model on over roughly 2,500 single-instrument, double-staffed MIDI files to determine both note and duration distributions. All of these MIDI files were sampled from PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing. This dataset contains over 250,000 public domain files for use in ML training.\n",
    "\n",
    "### Data Cleaning\n",
    "Processing the data required the use of multiple music packages. Initial attempts used <code>MidiUtil</code> and <code>Mido</code>, but since the model requires the use of a piano roll, <code>pretty_midi</code> was used for accessibility. While most MIDI files were cleaned up and ready for use, many had errors that pretty_midi was unable to solve. For example, a few files had a time scale of 128/4 that appeared as zeros. Any file that threw out an error was ignored in our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75388be2-3fd1-42d0-8ec5-4618648b9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19179fb-3c30-4769-8cb6-989c99714fbe",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "The algorithm I chose to employ was a Variational Autoencoder. The way this model works is best described as an hourglass-shaped neural network. This model works by taking as input a 2-channel piano roll of 128 possible pitches, multiplied by the number of notes we want to generate. Each note, pitch, and channel combination is called a node. The encoder goes through multiple layers, each with fewer nodes. These nodes capture key features that would be difficult to discover through manual feature extraction. Additionally, these nodes continue to decrease until they reach the latent dimension, the smallest layer with the most important features. The layers again increase until the output layer's size is the same as the input's shape. In short, **VAEs describe a probability distribution over the latent variables, whose encoder outputs the mean and variance of the distribution that is used to sample from the distribution and generate new notes.**\n",
    "\n",
    "The goal of using a VAE is to effectively learn patterns from multiple music samples. The result of running this algorithm is a set of features, or model weights, that best represent the sample files as a whole. Through each iteration of the model, a loss function calculates how well the model is related to a batch of randomly sampled training files. Ideally, the loss decreases with each epoch. Realistically, the loss will be very high, as each musical piece varies greatly through many features. A low loss usually means that the model is going to predict dissonant chords in an attempt to fill the most used notes positions. We will discuss methods used to prevent this from happening. We want to optimize our weights to produce a structurally sound song without generating a song that sounds slightly similar to every song it was trained on.\n",
    "\n",
    "### Parameters, Arguments, and Variables\n",
    "The following parameters and their selected arguments are described below:\n",
    "\n",
    "| Parameter | Description | Argument |\n",
    "| --------- | ----------- | -------- |\n",
    "| `seq_length` | Number of generated notes / Number of samples | 400 |\n",
    "| `input_dim` | Input dimension shape | 2 * 128 * `seq_length` |\n",
    "| `hidden_dim` | Encoder hidden dimension shape | 2048 |\n",
    "| `latent_dim` | Encoder latent dimension shape | 128 |\n",
    "| `output_dum` | Output dimension shape | `input_dim` |\n",
    "| `root_pitch` | Default pitch for generating notes | 60 |\n",
    "| `batch_size` | Number of samples processed in one iteration | 250 |\n",
    "| `num_samlpes` | Number of MIDI files to generate | 1 `or` 3 |\n",
    "| `max_notes_per_time` | Maximum chord size | 3 |\n",
    "\n",
    "This model also uses the following variable values:\n",
    "\n",
    "| Variable | Description | Value |\n",
    "| -------- | ----------- | ----- |\n",
    "| `max_duration` | Maximum note duration allowed | 50 |\n",
    "| `num_epochs` | Number of epochs in the loss function | 2 |\n",
    "| `threshold` | Probability threshold to qualify for candidacy | 0.035 |\n",
    "| `max_history` | Maximum number of repeated notes | 4 |\n",
    "| `offset` | Pitch shift change | 0 `or` 2 |\n",
    "| `note_duration` | Default note duration | .25 (eighth note) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42e465d-116a-41b5-9208-cb3443834e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation: Convert MIDI to a sequence representation with note durations\n",
    "def midi_to_sequence(midi_path, seq_length):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {midi_path}: {e}\")\n",
    "        # Return empty or dummy data if load fails\n",
    "        piano_roll = np.zeros((128, seq_length))\n",
    "        duration_array = np.zeros((128, seq_length))\n",
    "        return piano_roll, duration_array\n",
    "    piano_roll = pm.get_piano_roll(fs=20)  # 20 frames per second\n",
    "    # Binarize piano roll\n",
    "    piano_roll = (piano_roll > 0).astype(np.float32)\n",
    "\n",
    "    # Generate note duration information\n",
    "    duration_array = np.zeros_like(piano_roll)\n",
    "\n",
    "    for pitch in range(128):\n",
    "        pitch_vector = piano_roll[pitch, :]\n",
    "        diff = np.diff(pitch_vector, prepend=0)\n",
    "        onsets = np.where(diff == 1)[0]\n",
    "        offsets = np.where(diff == -1)[0]\n",
    "        if len(offsets) < len(onsets):\n",
    "            offsets = np.append(offsets, len(pitch_vector))\n",
    "        for onset, offset in zip(onsets, offsets):\n",
    "            duration = offset - onset\n",
    "            duration_array[pitch, onset:offset] = duration\n",
    "            \n",
    "    # Pad or truncate to fixed length\n",
    "    if piano_roll.shape[1] < seq_length:\n",
    "        pad_width = seq_length - piano_roll.shape[1]\n",
    "        piano_roll = np.pad(piano_roll, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        duration_array = np.pad(duration_array, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        piano_roll = piano_roll[:, :seq_length]\n",
    "        duration_array = duration_array[:, :seq_length]\n",
    "\n",
    "    return piano_roll, duration_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c73303-f0c4-4199-a988-491e27e06174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset class\n",
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, midi_files, seq_length=400):\n",
    "        self.files = midi_files\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        midi_path = self.files[idx]\n",
    "        piano_roll, duration_array = midi_to_sequence(midi_path, self.seq_length)\n",
    "        max_duration = 50\n",
    "        duration_norm = np.clip(duration_array / max_duration, 0, 1)\n",
    "        # Stack piano roll and duration as channels\n",
    "        # Shape: (2, pitch, time)\n",
    "        sample = np.stack([piano_roll, duration_norm], axis=0)\n",
    "        return torch.tensor(sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de1e983-a3ea-4fad-96b2-9dc523263842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define VAE components with dual outputs\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_length=400):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.output_dim = output_dim\n",
    "        # Split into two heads: one for pitch, one for duration\n",
    "        self.fc_pitch = nn.Linear(hidden_dim, 128 * seq_length)  # pitch output\n",
    "        self.fc_duration = nn.Linear(hidden_dim, 128 * seq_length)  # duration output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        pitch_logits = self.fc_pitch(h)\n",
    "        duration_logits = self.fc_duration(h)\n",
    "        # reshape to (batch, channels=2, pitch=128, time=seq_length)\n",
    "        pitch_logits = pitch_logits.view(-1, 128, self.seq_length)\n",
    "        duration_logits = duration_logits.view(-1, 128, self.seq_length)\n",
    "        # Sigmoid for pitch (binary presence)\n",
    "        pitch_probs = self.sigmoid(pitch_logits)\n",
    "        return pitch_probs, duration_logits\n",
    "\n",
    "class MusicVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        pitch_probs, duration_logits = self.decoder(z)\n",
    "        return pitch_probs, duration_logits, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9b9ff7-e9c3-4f57-a55a-ecf56dc5e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loss function with dual components\n",
    "def loss_function(pitch_probs, duration_logits, x, mu, logvar):\n",
    "    # Split x into target pitch and duration\n",
    "    target_pitch = x[:, 0, :, :]  # shape: (batch, 128, time)\n",
    "    target_duration = x[:, 1, :, :]  # shape: (batch, 128, time)\n",
    "    max_duration = 50\n",
    "\n",
    "    # Compute binary cross-entropy for pitch\n",
    "    BCE = nn.functional.binary_cross_entropy(pitch_probs, target_pitch, reduction='sum')\n",
    "    # Compute MSE for durations (regression)\n",
    "    duration_pred = duration_logits\n",
    "    duration_target = target_duration\n",
    "    # Denormalize durations for loss calculation if desired\n",
    "    MSE = nn.functional.mse_loss(duration_pred, duration_target, reduction='sum')\n",
    "    # KLD for VAE\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + MSE + KLD\n",
    "\n",
    "C_MAJOR_SCALE = [0, 2, 4, 5, 7, 9, 11]\n",
    "def pitch_shift_to_c_major(pitch, base_pitch=60):\n",
    "    pitch_in_scale = pitch - base_pitch\n",
    "    # Find the closest scale step\n",
    "    distances = [abs(pitch_in_scale - interval) for interval in C_MAJOR_SCALE]\n",
    "    min_index = np.argmin(distances)\n",
    "    shifted_pitch = base_pitch + C_MAJOR_SCALE[min_index]\n",
    "    return shifted_pitch\n",
    "\n",
    "CHORD_PATTERNS = [\n",
    "    [0, 4, 7],       # Major triad\n",
    "    [-3, 0, 4],      # Minor triad1\n",
    "    [-7, -3, 0],     # Minor triad2\n",
    "    [-5, -1, 2]      # Other chord\n",
    "]\n",
    "def get_custom_chord(root_pitch):\n",
    "    # Randomly select one of your custom chord patterns\n",
    "    pattern = CHORD_PATTERNS[np.random.randint(len(CHORD_PATTERNS))]\n",
    "    # Transpose pattern to the root pitch\n",
    "    chord_pitches = [root_pitch + interval for interval in pattern]\n",
    "    # Keep within MIDI pitch range\n",
    "    chord_pitches = [p for p in chord_pitches if 0 <= p <= 127]\n",
    "    return chord_pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "94584f47-a4bd-4a81-9b9e-c0474d0ef3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 1/2, Loss: 21945.138613672698\n",
      "Epoch 2/2, Loss: 19212.82993596717\n",
      "Finished Training in 371.63s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 5. Setup training\n",
    "midi_files = (glob(\"mid/0/0/*.mid\") +\n",
    "              glob(\"mid/0/1/*.mid\") + \n",
    "              glob(\"mid/0/2/*.mid\") + \n",
    "              glob(\"mid/0/3/*.mid\") + \n",
    "              glob(\"mid/0/4/*.mid\"))\n",
    "dataset = MidiDataset(midi_files, seq_length=400)\n",
    "dataloader = DataLoader(dataset, batch_size=250, shuffle=True, num_workers=0)\n",
    "\n",
    "input_dim = 2 * 128 * 400  # 2 channels: piano roll + duration\n",
    "hidden_dim = 2048\n",
    "latent_dim = 128\n",
    "\n",
    "model = MusicVAE(input_dim, hidden_dim, latent_dim, seq_length=400)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Start Training\")\n",
    "t1 = time.time()\n",
    "# 6. Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.view(batch.size(0), -1)  # flatten to (batch, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        pitch_probs, duration_logits, mu, logvar = model(batch)\n",
    "        loss = loss_function(pitch_probs, duration_logits, batch.view(batch.size(0), 2, 128, -1), mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "t2 = time.time()\n",
    "print(f\"Finished Training in {round(t2 - t1, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9841813-6445-4686-a61b-2a8b3ae6633c",
   "metadata": {},
   "source": [
    "### Music Generation\n",
    "Our model then generates notes, either single notes or chords, based on the probability of that note being played. The way this works is simple: the VAE determines a probability distribution of every possible note that was played within a certain beat. The algorithm filters out any note with a probability below the threshold value. Then, it checks the length of this array. If there are more than three notes, there is a high probability of generating a dissonant chord; therefore, the three notes with the greatest probability are selected. If there are 1 or 2 notes, randomly select one. If no notes were above the threshold, generate the previous note. This is important so as to reduce randomness, increase tone harmonics, and produce a more audibly pleasing composition. Other constraints, such as limiting repeating notes to 4 consecutive notes, are also used to reduce noise and improve our generation capabilities. \n",
    "\n",
    "One advantage of implementing the model this way is that it prevents bias in our model. By limiting how often the most popular or frequent note is played, our model has a reduced risk of predicting the same note for all possible positions. A disadvantage of this method is that more advanced models that can benefit from larger datasets are available, but due to our limited time and memory constraints, a simpler model was chosen. Additionally, this model heavily predicts chords over single notes. The reasons why this happens are twofold. First, a low threshold value means that more candidate notes are chosen, so chords are more likely to be played. Second, this is due to the training dataset containing a significantly large number of chords, making single note generation more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46033a00-9f3b-4bd7-8f9d-303d74561f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generate new music with note durations\n",
    "def generate_music_with_repetition_control(model, latent_dim, num_samples=1, seq_length=400, max_notes_per_time=3):\n",
    "    threshold= 0.035\n",
    "    max_history = 4  # number of previous notes to compare\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        pitch_probs, duration_logits = model.decoder(z)\n",
    "        pitch_probs = pitch_probs.cpu().numpy()\n",
    "        duration_logits = duration_logits.cpu().numpy()\n",
    "    \n",
    "        for i in range(num_samples):\n",
    "            pm = pretty_midi.PrettyMIDI()\n",
    "            instrument = pretty_midi.Instrument(program=0)\n",
    "            dur_pred = duration_logits[i]\n",
    "            dur_pred = np.clip(dur_pred, 0, 1)\n",
    "    \n",
    "            previous_notes = [[60]]\n",
    "\n",
    "            start_time = 0\n",
    "            for t in range(seq_length):\n",
    "                selected_pitches = []\n",
    "                pitch_probs_t = pitch_probs[i, :, t]\n",
    "                pitch_probs_t = np.array(pitch_probs_t) / sum(pitch_probs_t)\n",
    "                candidate_pitches = np.where(pitch_probs_t > threshold)[0].tolist()\n",
    "                if len(candidate_pitches) == 0:\n",
    "                    last_pitch = previous_notes[-1][-1]\n",
    "                    shifted_pitch = pitch_shift_to_c_major(last_pitch)\n",
    "                    selected_pitches.append(shifted_pitch)\n",
    "                elif len(candidate_pitches) > max_notes_per_time:\n",
    "                    # Pick a root pitch from candidates\n",
    "                    root_pitch = np.random.choice(candidate_pitches)\n",
    "                    # Generate the major chord pitches\n",
    "                    chord_pitches = get_custom_chord(root_pitch)\n",
    "                    # Assign the chord pitches as the selected notes\n",
    "                    selected_pitches.extend(chord_pitches)\n",
    "                else:\n",
    "                    choose = np.random.choice(candidate_pitches)\n",
    "                    shifted_pitch = pitch_shift_to_c_major(choose)\n",
    "                    selected_pitches.append(shifted_pitch)\n",
    "\n",
    "                offset = 0\n",
    "                if all(selected_pitches == i for i in previous_notes):\n",
    "                    offset = 2\n",
    "                for pitch_lst in selected_pitches:\n",
    "                    if type(pitch_lst) != list:\n",
    "                        pitch_lst = [pitch_lst]\n",
    "                    for p in pitch_lst:\n",
    "                        note_duration = dur_pred[p, t]\n",
    "                        if note_duration == 0:\n",
    "                            note_duration = .25\n",
    "                        end_time = start_time + note_duration \n",
    "                        note = pretty_midi.Note(velocity=100, pitch=p + offset, start=start_time, end=end_time)\n",
    "                        instrument.notes.append(note)\n",
    "                        \n",
    "                if all(selected_pitches == i for i in previous_notes):\n",
    "                    selected_pitches = np.array(selected_pitches)\n",
    "                    selected_pitches += offset\n",
    "                    selected_pitches = list(selected_pitches)\n",
    "                previous_notes.append(selected_pitches)\n",
    "                \n",
    "                if len(previous_notes) > max_history:\n",
    "                    previous_notes = previous_notes[-max_history:]\n",
    "                start_time = end_time\n",
    "\n",
    "            pm.instruments.append(instrument)\n",
    "            pm.write(f\"music/1d_VAE_{i}.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8043df66-3b16-40f6-9ce4-8e58ffa07a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Generate and save new MIDI with note durations\n",
    "generate_music_with_repetition_control(model, latent_dim, num_samples=3, seq_length=400)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdab3f-e7f5-4f81-9ba1-d33e55d079cd",
   "metadata": {},
   "source": [
    "### Results\n",
    "While our generated music wont sound like any of our training data, our baseline would have to be a sample of our training data. To evaluate what a \"good\" output is, we would need to perform a Subjective Listening Test, either using a mean opinion score. Listeners would have to rate the quality of the composition in comparison to the training data, or a related composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b5a45-e8b2-4850-adf4-af613ea2d393",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Task 1b - 2D Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3e5ef-ccb0-471c-9f20-3f3720ee2b7e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "One major disadvantage of the previous model is its dimensionality. Attempting to generate 400 notes results in an input size of 102,400 Nodes! This can be mitigated by using a 2-D VAE. Since our piano rolls are 2-dimensional (where rows are pitches and columns are time intervals), we can instead opt to create an encoder with <code>Conv2d</code>. Our node size decreases by half, and our runtimes are improved. For our first model, an epoch would take roughly 3 minutes to complete. Our new model takes roughly 37 seconds per epoch.\n",
    "\n",
    "### Parameters, Arguments, and Variables\n",
    "The following parameters and their selected arguments have changed compared to the first model:\n",
    "\n",
    "| Parameter | Description | Argument |\n",
    "| --------- | ----------- | -------- |\n",
    "| `input_dim` | Deprecated, is now `self._to_linear` | 256 * 8 * 25 |\n",
    "\n",
    "This model also chaged the following variable values:\n",
    "\n",
    "| Variable | Description | Value |\n",
    "| -------- | ----------- | ----- |\n",
    "| `num_epochs` | Number of epochs in the loss function | 35 `or` early stopping |\n",
    "| `threshold` | Probability threshold to qualify for candidacy | 0.04 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9da9aa-42ad-44c5-97ec-4f971be18fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgar\\AppData\\Roaming\\Python\\Python313\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Loss: 70979369.0\n",
      "Epoch 2/35, Loss: 11610682.6\n",
      "Epoch 3/35, Loss: 4469826.95\n",
      "Epoch 4/35, Loss: 4029529.2\n",
      "Epoch 5/35, Loss: 12916944.1\n",
      "Early stopping triggered after epoch 5\n",
      "Finished Training in 222.45s\n"
     ]
    }
   ],
   "source": [
    "# Steps 1, 2, 4, and 7 are unmodified, and are not reproduced here for simplicity\n",
    "\n",
    "# 3. Define 2D Convolutional Encoder with BatchNorm and Dropout\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=(3, 3), stride=2, padding=1),  # Output: (32, 64, 200)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=2, padding=1), # (64, 32, 100)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=1), # (128, 16, 50)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=2, padding=1), # (256, 8, 25)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "\n",
    "        # Compute the flattened size after convolutions\n",
    "        self._to_linear = 256 * 8 * 25  # based on above dimensions\n",
    "\n",
    "        self.fc_mu = nn.Linear(self._to_linear, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self._to_linear, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.conv_layers(x)  # shape: (batch, 256, 8, 25)\n",
    "        x = x.view(batch_size, -1)  # flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_length=400):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.output_dim = output_dim\n",
    "        # Split into two heads: one for pitch, one for duration\n",
    "        self.fc_pitch = nn.Linear(hidden_dim, 128 * seq_length)  # pitch output\n",
    "        self.fc_duration = nn.Linear(hidden_dim, 128 * seq_length)  # duration output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        pitch_logits = self.fc_pitch(h)\n",
    "        duration_logits = self.fc_duration(h)\n",
    "        # reshape to (batch, channels=2, pitch=128, time=seq_length)\n",
    "        pitch_logits = pitch_logits.view(-1, 128, self.seq_length)\n",
    "        duration_logits = duration_logits.view(-1, 128, self.seq_length)\n",
    "        # Sigmoid for pitch (binary presence)\n",
    "        pitch_probs = self.sigmoid(pitch_logits)\n",
    "        return pitch_probs, duration_logits\n",
    "\n",
    "class MusicVAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        # Keep your decoder as is or modify similarly\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, output_dim=2*128*seq_length, seq_length=seq_length)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        pitch_probs, duration_logits = self.decoder(z)\n",
    "        return pitch_probs, duration_logits, mu, logvar\n",
    "\n",
    "# 5. Setup training\n",
    "midi_files = (glob(\"mid/0/0/*.mid\") +\n",
    "              glob(\"mid/0/1/*.mid\") + \n",
    "              glob(\"mid/0/2/*.mid\") + \n",
    "              glob(\"mid/0/3/*.mid\") + \n",
    "              glob(\"mid/0/4/*.mid\"))\n",
    "dataset = MidiDataset(midi_files, seq_length=400)\n",
    "dataloader = DataLoader(dataset, batch_size=250, shuffle=True, num_workers=0)\n",
    "\n",
    "input_dim = 2 * 128 * 400  # 2 channels: piano roll + duration\n",
    "hidden_dim = 2048\n",
    "latent_dim = 128\n",
    "\n",
    "model = MusicVAE(hidden_dim, latent_dim, seq_length=400)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Start Training\")\n",
    "t1 = time.time()\n",
    "best_loss = float('inf')\n",
    "loss_increased = 0\n",
    "\n",
    "# To store the best model\n",
    "best_model_state = None\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pitch_probs, duration_logits, mu, logvar = model(batch)\n",
    "        loss = loss_function(pitch_probs, duration_logits, batch.view(batch.size(0), 2, 128, -1), mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        loss_increased = 1\n",
    "    if loss_increased:\n",
    "        print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "t2 = time.time()\n",
    "print(f\"Finished Training in {round(t2 - t1, 2)}s\")\n",
    "\n",
    "# Load the best model after training\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8c53-b51b-440a-a2ce-467864b706b1",
   "metadata": {},
   "source": [
    "### Results\n",
    "To improve compositional clarity, sampled single notes are evaluated in the C-major scale.\n",
    "\n",
    "While the loss function results in a greater value than the previous model, the MIDI files generated sound clearer and more structurally sound. This can be thought of as reducing overfitting on the training dataset to better generalize to unseen music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1cfec3-aea9-49be-8101-c8dd117c85a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "def generate_music_with_repetition_control(model, latent_dim, num_samples=1, seq_length=400, max_notes_per_time=3):\n",
    "    threshold= 0.04\n",
    "    max_history = 4  # number of previous notes to compare\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        pitch_probs, duration_logits = model.decoder(z)\n",
    "        pitch_probs = pitch_probs.cpu().numpy()\n",
    "        duration_logits = duration_logits.cpu().numpy()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            pm = pretty_midi.PrettyMIDI()\n",
    "            instrument = pretty_midi.Instrument(program=0)\n",
    "            dur_pred = duration_logits[i]\n",
    "            dur_pred = np.clip(dur_pred, 0, 1)\n",
    "\n",
    "            previous_notes = [[60]]\n",
    "\n",
    "            start_time = 0\n",
    "            for t in range(seq_length):\n",
    "                # Retrieve pitch probabilities at time t\n",
    "                pitch_probs_t = pitch_probs[i, :, t]\n",
    "                total_prob = np.sum(pitch_probs_t)\n",
    "                pitch_probs_t = pitch_probs_t / total_prob\n",
    "                # Candidate pitches above threshold\n",
    "                candidate_pitches = np.where(pitch_probs_t > threshold)[0].tolist()\n",
    "                candidate_probs = pitch_probs_t[candidate_pitches]\n",
    "                # If no candidate pitches, shift last pitch\n",
    "                if len(candidate_pitches) == 0:\n",
    "                    last_pitch = previous_notes[-1][-1]\n",
    "                    shifted_pitch = pitch_shift_to_c_major(last_pitch)\n",
    "                    selected_pitches = [shifted_pitch]\n",
    "                elif len(candidate_pitches) >= max_notes_per_time:\n",
    "                    # Select top 'max_notes_per_time' pitches based on probability\n",
    "                    # Get indices of top probabilities\n",
    "                    top_indices = np.argsort(candidate_probs)[-max_notes_per_time:][::-1]\n",
    "                    selected_pitches = [candidate_pitches[idx] for idx in top_indices]\n",
    "                else:\n",
    "                    # Random choice among candidates\n",
    "                    choose_idx = np.random.choice(len(candidate_pitches))\n",
    "                    choose_pitch = candidate_pitches[choose_idx]\n",
    "                    # shifted_pitch = pitch_shift_to_c_major(choose_pitch)\n",
    "                    selected_pitches = [choose_pitch]\n",
    "\n",
    "                offset = 0\n",
    "                # Check for repetition\n",
    "                if all(selected_pitches == i for i in previous_notes):\n",
    "                    offset = 2\n",
    "\n",
    "                # Append notes to MIDI\n",
    "                end_time = start_time\n",
    "                for p in selected_pitches:\n",
    "                    note_duration = dur_pred[p, t]\n",
    "                    if note_duration == 0:\n",
    "                        note_duration = 0.25\n",
    "                    end_time = start_time + note_duration\n",
    "                    note = pretty_midi.Note(velocity=100, pitch=p + offset, start=start_time, end=end_time)\n",
    "                    instrument.notes.append(note)\n",
    "\n",
    "                # Prepare for next iteration\n",
    "                if all(selected_pitches == i for i in previous_notes):\n",
    "                    selected_pitches = np.array(selected_pitches) + offset\n",
    "                    selected_pitches = list(selected_pitches)\n",
    "\n",
    "                previous_notes.append(selected_pitches)\n",
    "                if len(previous_notes) > max_history:\n",
    "                    previous_notes = previous_notes[-max_history:]\n",
    "                start_time = end_time\n",
    "\n",
    "            pm.instruments.append(instrument)\n",
    "            pm.write(f\"music/2d_VAE_{i}.mid\")\n",
    "\n",
    "# Usage (assuming model and functions are defined)\n",
    "generate_music_with_repetition_control(model, latent_dim, num_samples=3, seq_length=400)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26a489-e923-4f1f-8e22-688b3f849dfb",
   "metadata": {},
   "source": [
    "# Task 2 - Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d0f39-a946-4c65-8393-e22dce57d8a9",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Our second model continues our use of MIDI files, this time for symbolic conditioned generation. This time, the model aims to generate music given a music scale, as well as a keyword to determine a chord progression to be used. The goal of this model will be to generate music that harmonizes; one that generates notes and chords following a scale pattern. For example, the model can generate a \"vi IV I V F Major Hopeful\" composition. We are also able to train our model using the entirety of the PDMX dataset, as well as another publicly available dataset: Maestro. Maestro contains over 1000 MIDI files that are longer and more robust than those of PDMX. Most importantly, these compositions contain very few chords, giving us more features related to single note generation.\n",
    "\n",
    "### Data Cleaning\n",
    "Given that over 250,000 files were used to train our model, caching results was critical to improve consecutive runtimes. 14 different JSON files are used, each taking over half an hour to create, but  only seconds to load in. For this model, MIDITok and symusic were the primary packages used to process the data. No files were left out, as these packages were able to properly extract all the necessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56471a-53b5-4c6f-b3a1-f2e155176714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from midiutil import MIDIFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb849f53-0c9d-4630-90af-ac72536ee920",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a57d8a1-5012-4eb6-a37b-663713bd330f",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "The algorithm I chose to implement for this model was an expanded Markov Chain, using n-gram probabilities to generate both pitch and duration for each note. The model determines the next note to play based on a random choice, influenced by the probabilities of the trigrams. Simply put: this algorithm predicts the next musical element based on the current state and its transitional probabilities. The goal of a Markov chain is to increase stylistic coherence. This model differs from the VAE used in Task 1a and 1b as there is no loss function. This becomes a problem as Markov chains can follow a pattern that diverges from the original composition. To prevent this, a few constraints are included. First, since most of the music is in 4/4 time scale, we constrain the generations to beats that conform to this time scale. Additionally, notes not belonging to the given input scale are not used. Finally, keywords determine chord progression that plays on every first beat of a scale. All of these features improve the structural coherence of the composition. Advantages of this model are its ability to train on larger datasets with minimal memory usage, while a disadvantage is its simplicity limits how accurate, coherent, or audibly appealing a composition can become.\n",
    "\n",
    "Similar to our previous task model, our generated music will be evaluated through Subjective Learning tests. Additionally, since this model is simpler and can possibly generate more coherent music by following more basic conditions, we can also determine audio diversity, and generate an Inception Score from our training data, or from an outside composition. During comparison, features such as chord similarity would improve our score. Baselines for this task would include whether the generated composition effectively follows a sequence of notes, or if its distribution of chords is similar to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e246f11-6c39-461a-9ffb-4dc8dc604d56",
   "metadata": {},
   "source": [
    "**Warning:** Due to GitHub's file size restrictions, <code>beat_extractions.json</code> (466 MB) and <code>note_extractions.json</code> (231 MB) are not pushed to the project page. Additionally, <code>mid</code>(1.79 GB), the folder that contains all 250,000+ MIDI files, is available for download at https://github.com/pnlong/PDMX/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d294b-3218-45b7-92f4-e7e4932d0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect MIDI files\n",
    "midi_files = []\n",
    "for i in range(18):\n",
    "    for j in range(58):\n",
    "        mids = glob(f\"mid/{i}/{j}/*.mid\")\n",
    "        midi_files += mids\n",
    "for i in range(10):\n",
    "    mids = glob(f\"maestro-v3.0.0/{i}/*.midi\")\n",
    "    midi_files += mids\n",
    "    \n",
    "config = TokenizerConfig(num_velocities=1, use_chords=False, use_programs=False)\n",
    "tokenizer = REMI(config)\n",
    "    \n",
    "# Check if the tokenizer already exists\n",
    "if os.path.exists(\"cache/tokenizer.json\"):\n",
    "    tokenizer.from_pretrained(\"cache/tokenizer.json\")\n",
    "else:\n",
    "    tokenizer.train(vocab_size=1000, files_paths=midi_files)\n",
    "    tokenizer.save(\"cache/tokenizer.json\")\n",
    "    \n",
    "tok_time = time.time()\n",
    "print(f\"Task finished in {round(tok_time - start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80e320-67b7-430d-a51f-9cd253cec30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g.:\n",
    "midi = Score(midi_files[0])\n",
    "tokens = tokenizer(midi)[0].tokens\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a214336-b0ec-4709-9f87-54d0d12b1dd2",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc1416-a4de-456c-96ca-aabaee99164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "def note_extraction(midi_file):\n",
    "    # Q1a: Your code goes here\n",
    "    midi = Score(midi_file)\n",
    "    tokens = tokenizer(midi)[0].tokens\n",
    "    valid = [i for i in tokens if \"Pitch\" in i]\n",
    "    nums = [int(j.split(\"_\")[-1]) for j in valid]\n",
    "\n",
    "    return nums\n",
    "\n",
    "def note_frequency(midi_files):\n",
    "    x = 0\n",
    "    CACHE_FILE = 'cache/note_frequencies.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    if x:\n",
    "        return x\n",
    "        \n",
    "    # Q1b: Your code goes here\n",
    "    freqs = defaultdict(int)\n",
    "    for i in midi_files:\n",
    "        nums = note_extraction(i)\n",
    "        for j in nums:\n",
    "            freqs[j] += 1\n",
    "            \n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(freqs)))\n",
    "            \n",
    "    return dict(freqs)\n",
    "\n",
    "CACHE_FILE_3 = 'cache/note_extractions.json'\n",
    "if os.path.exists(CACHE_FILE_3):\n",
    "    with open(CACHE_FILE_3, 'r') as f:\n",
    "        note_extractions = json.load(f)\n",
    "else:\n",
    "    note_extractions = [note_extraction(i) for i in midi_files]\n",
    "    with open(CACHE_FILE_3, 'w') as f:\n",
    "        json.dump(note_extractions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904b67f-bafd-4b80-b2d2-be27a6e051f6",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccee1d8-7241-4339-a346-6b87a53bb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "def note_unigram_probability(midi_files):\n",
    "    note_counts = note_frequency(midi_files)\n",
    "    unigramProbabilities = note_counts.copy()\n",
    "    vals = 0\n",
    "\n",
    "    # Q2: Your code goes here\n",
    "    for i in unigramProbabilities.values():\n",
    "        vals += i\n",
    "    for i in unigramProbabilities:\n",
    "        unigramProbabilities[i] /= vals\n",
    "    \n",
    "    return unigramProbabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca16d82-12c8-406e-93f9-dd5812173d5c",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cbc75-9bf0-460c-aac6-f15cd0defb9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Q3\n",
    "def note_bigram_probability(midi_files):    \n",
    "    x, y = 0, 0\n",
    "    CACHE_FILE = 'cache/bigram_transitions.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    \n",
    "    CACHE_FILE_2 = 'cache/bigram_transition_probabilities.json'\n",
    "    if os.path.exists(CACHE_FILE_2):\n",
    "        with open(CACHE_FILE_2, 'r') as f:\n",
    "            content = f.read()\n",
    "        y = ast.literal_eval(content)\n",
    "        \n",
    "    if x and y:\n",
    "        return x, y\n",
    "\n",
    "    bigramTransitions = defaultdict(list)\n",
    "    bigramTransitionProbabilities = defaultdict(list)\n",
    "    \n",
    "\n",
    "    for files in note_extractions:\n",
    "        for i in range(len(files) - 1):\n",
    "            first = files[i]\n",
    "            last = files[i + 1]\n",
    "            bigramTransitions[first].append(last)\n",
    "\n",
    "    for key, vals in bigramTransitions.items():\n",
    "        counts = defaultdict(int)\n",
    "        for note in vals:\n",
    "            counts[note] += 1\n",
    "        total = sum(counts.values())\n",
    "        probs = []\n",
    "        uniques = []\n",
    "        for note, count in counts.items():\n",
    "            uniques.append(note)\n",
    "            probs.append(count / total)\n",
    "        bigramTransitionProbabilities[key] = probs\n",
    "        bigramTransitions[key] = uniques\n",
    "        \n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(bigramTransitions)))\n",
    "    with open(CACHE_FILE_2, 'w') as f:\n",
    "        f.write(repr(dict(bigramTransitionProbabilities)))\n",
    "\n",
    "    return bigramTransitions, bigramTransitionProbabilities\n",
    "        \n",
    "x, y = note_bigram_probability(midi_files)\n",
    "def sample_next_note(note):\n",
    "    # Q3b: Your code goes here\n",
    "    max_value = max(y[note])\n",
    "    max_index = y[note].index(max_value)\n",
    "    \n",
    "    return x[note][max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fc645-e5be-4cdb-a7af-0f723f21f0fd",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2a2f6-f4b5-4f32-81b8-efd7ba1a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "def note_trigram_probability(midi_files):\n",
    "    x, y = 0, 0\n",
    "    CACHE_FILE = 'cache/trigram_transitions.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    \n",
    "    CACHE_FILE_2 = 'cache/trigram_transition_probabilities.json'\n",
    "    if os.path.exists(CACHE_FILE_2):\n",
    "        with open(CACHE_FILE_2, 'r') as f:\n",
    "            content = f.read()\n",
    "        y = ast.literal_eval(content)\n",
    "        \n",
    "    if x and y:\n",
    "        return x, y\n",
    "\n",
    "    trigramTransitions = defaultdict(list)\n",
    "    trigramTransitionProbabilities = defaultdict(list)\n",
    "\n",
    "    # Q5a: Your code goes here\n",
    "    trigramTransitionCounts = defaultdict(lambda: defaultdict(int))\n",
    "    totalCounts = defaultdict(int)\n",
    "\n",
    "    for files in note_extractions:\n",
    "        files = np.array(files)\n",
    "        if len(files) < 3:\n",
    "            continue\n",
    "        for i in range(2, len(files)):\n",
    "            prev2 = files[i - 2]\n",
    "            prev1 = files[i - 1]\n",
    "            curr = files[i]\n",
    "            key = (prev2, prev1)\n",
    "            trigramTransitionCounts[key][curr] += 1\n",
    "            totalCounts[key] += 1\n",
    "\n",
    "    for key in trigramTransitionCounts:\n",
    "        total = totalCounts[key]\n",
    "        next_notes = list(trigramTransitionCounts[key].keys())\n",
    "        probs = [trigramTransitionCounts[key][n] / total for n in next_notes]\n",
    "        trigramTransitions[key] = next_notes\n",
    "        trigramTransitionProbabilities[key] = probs\n",
    "\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(trigramTransitions)))\n",
    "    with open(CACHE_FILE_2, 'w') as f:\n",
    "        f.write(repr(dict(trigramTransitionProbabilities)))\n",
    "    \n",
    "    return trigramTransitions, trigramTransitionProbabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0b6e4-b0fe-4162-8e25-faacf53b4a91",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e6654-da33-4303-b0c7-1157ccdb9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "duration2length = {\n",
    "    \"0.1.8\": 1, \n",
    "    \"0.2.8\": 2, # sixteenth note, 0.25 beat in 4/4 time signature\n",
    "    \"0.3.8\": 3,\n",
    "    \"0.4.8\": 4, # eighth note, 0.5 beat in 4/4 time signature\n",
    "    \"0.5.8\": 5,\n",
    "    \"0.6.8\": 6,\n",
    "    \"0.7.8\": 7, \n",
    "    \"1.0.8\": 8, # quarter note, 1 beat in 4/4 time signature\n",
    "    \"1.1.8\": 9, \n",
    "    \"1.2.8\": 10, \n",
    "    \"1.3.8\": 11, \n",
    "    \"1.4.8\": 12, \n",
    "    \"1.5.8\": 13, \n",
    "    \"1.6.8\": 14, \n",
    "    \"1.7.8\": 15, \n",
    "    \"2.0.8\": 16, # half note, 2 beats in 4/4 time signature\n",
    "    \"2.1.8\": 17, \n",
    "    \"2.2.8\": 18, \n",
    "    \"2.3.8\": 19, \n",
    "    \"2.4.8\": 20, \n",
    "    \"2.5.8\": 21, \n",
    "    \"2.6.8\": 22, \n",
    "    \"2.7.8\": 23, \n",
    "    \"3.0.8\": 24, \n",
    "    \"3.1.8\": 25, # 3 beats and 1/8 of a beat\n",
    "    \"3.2.8\": 26, \n",
    "    \"3.3.8\": 27, \n",
    "    \"3.4.8\": 28, \n",
    "    \"3.5.8\": 29, \n",
    "    \"3.6.8\": 30, \n",
    "    \"3.7.8\": 31, \n",
    "    \"4.0.4\": 32, # whole note, 4 beats in 4/4 time signature\n",
    "    \"4.1.4\": 34, \n",
    "    \"4.2.4\": 36, \n",
    "    \"4.3.4\": 38, # 4 beats and 3/4 of a beat\n",
    "    \"5.0.4\": 40, \n",
    "    \"5.1.4\": 42, \n",
    "    \"5.2.4\": 44, \n",
    "    \"5.3.4\": 46, \n",
    "    \"6.0.4\": 48, \n",
    "    \"6.1.4\": 50, \n",
    "    \"6.2.4\": 52, \n",
    "    \"6.3.4\": 54, \n",
    "    \"7.0.4\": 56, \n",
    "    \"7.1.4\": 58, \n",
    "    \"7.2.4\": 60, \n",
    "    \"7.3.4\": 62, \n",
    "    \"8.0.4\": 64, \n",
    "    \"8.1.4\": 66, \n",
    "    \"8.2.4\": 68, \n",
    "    \"8.3.4\": 70, \n",
    "    \"9.0.4\": 72, \n",
    "    \"9.1.4\": 74, \n",
    "    \"9.2.4\": 76, \n",
    "    \"9.3.4\": 78, \n",
    "    \"10.0.4\": 80, \n",
    "    \"10.1.4\": 82, \n",
    "    \"10.2.4\": 84, \n",
    "    \"10.3.4\": 86, \n",
    "    \"11.0.4\": 88, \n",
    "    \"11.1.4\": 90, \n",
    "    \"11.2.4\": 92, \n",
    "    \"11.3.4\": 94, \n",
    "    \"12.0.4\": 96, \n",
    "}\n",
    "\n",
    "def beat_extraction(midi_file):\n",
    "    # Q6: Your code goes here\n",
    "    midi = Score(midi_file)\n",
    "    tokens = tokenizer(midi)[0].tokens\n",
    "    pos_dur = [i.split(\"_\")[-1] for i in tokens if \"Position\" in i or \"Duration\" in i]\n",
    "    pos_len = [duration2length[i] if i in duration2length.keys() else int(i) for i in pos_dur]\n",
    "    beats = list(zip(pos_len[::2], pos_len[1::2]))\n",
    "    return beats\n",
    "\n",
    "CACHE_FILE = \"cache/beat_extractions.json\"\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, 'r') as f:\n",
    "        content = json.load(f)\n",
    "    beat_extractions = []\n",
    "    for i in content:\n",
    "        beats = []\n",
    "        for j, k in i:\n",
    "            beats.append(tuple([int(j), int(k)]))\n",
    "        beat_extractions.append(beats)\n",
    "    content = 0\n",
    "else:\n",
    "    beat_extractions = [beat_extraction(i) for i in midi_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70d584-6ed1-481a-a1c3-61061a916aa5",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093d056-f823-4658-ba3b-57e0db6dfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "def beat_bigram_probability(midi_files):\n",
    "    x, y = 0, 0\n",
    "    CACHE_FILE = 'cache/bigram_beat_transitions.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    \n",
    "    CACHE_FILE_2 = 'cache/bigram_beat_transition_probabilities.json'\n",
    "    if os.path.exists(CACHE_FILE_2):\n",
    "        with open(CACHE_FILE_2, 'r') as f:\n",
    "            content = f.read()\n",
    "        y = ast.literal_eval(content)\n",
    "        \n",
    "    if x and y:\n",
    "        return x, y\n",
    "        \n",
    "    bigramBeatTransitions = defaultdict(list)\n",
    "    bigramBeatTransitionProbabilities = defaultdict(list)\n",
    "\n",
    "    # Q7: Your code goes here\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for beats in beat_extractions:\n",
    "        beat_lengths = [length for (_, length) in beats]\n",
    "\n",
    "        for i in range(len(beat_lengths) - 1):\n",
    "            prev_length = beat_lengths[i]\n",
    "            curr_length = beat_lengths[i + 1]\n",
    "            transition_counts[prev_length][curr_length] += 1\n",
    "\n",
    "    for prev_length, next_lengths_counts in transition_counts.items():\n",
    "        total = sum(next_lengths_counts.values())\n",
    "        probabilities = []\n",
    "        next_lengths = []\n",
    "        for length, count in next_lengths_counts.items():\n",
    "            next_lengths.append(length)\n",
    "            probabilities.append(count / total)\n",
    "        bigramBeatTransitions[prev_length] = next_lengths\n",
    "        bigramBeatTransitionProbabilities[prev_length] = probabilities\n",
    "\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(bigramBeatTransitions)))\n",
    "    with open(CACHE_FILE_2, 'w') as f:\n",
    "        f.write(repr(dict(bigramBeatTransitionProbabilities)))\n",
    "\n",
    "    return bigramBeatTransitions, bigramBeatTransitionProbabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5da214-5339-4a68-ac36-79a90a496524",
   "metadata": {},
   "source": [
    "### Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc42f2-0e46-44e2-b32d-a639eb618e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "def beat_pos_bigram_probability(midi_files):\n",
    "    x, y = 0, 0\n",
    "    CACHE_FILE = 'cache/bigram_beat_pos_transitions.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    \n",
    "    CACHE_FILE_2 = 'cache/bigram_beat_pos_transition_probabilities.json'\n",
    "    if os.path.exists(CACHE_FILE_2):\n",
    "        with open(CACHE_FILE_2, 'r') as f:\n",
    "            content = f.read()\n",
    "        y = ast.literal_eval(content)\n",
    "        \n",
    "    if x and y:\n",
    "        return x, y\n",
    "        \n",
    "    bigramBeatPosTransitions = defaultdict(list)\n",
    "    bigramBeatPosTransitionProbabilities = defaultdict(list)\n",
    "\n",
    "    # Q8a: Your code goes here\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for beats in beat_extractions:\n",
    "        for i in range(len(beats)):\n",
    "            curr_pos, curr_length = beats[i]\n",
    "            transition_counts[curr_pos][curr_length] += 1\n",
    "    \n",
    "    for pos, next_lengths_counts in transition_counts.items():\n",
    "        total = sum(next_lengths_counts.values())\n",
    "        next_lengths = []\n",
    "        probs = []\n",
    "        for length, count in next_lengths_counts.items():\n",
    "            next_lengths.append(length)\n",
    "            probs.append(count / total)\n",
    "        bigramBeatPosTransitions[pos] = next_lengths\n",
    "        bigramBeatPosTransitionProbabilities[pos] = probs\n",
    "\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(bigramBeatPosTransitions)))\n",
    "    with open(CACHE_FILE_2, 'w') as f:\n",
    "        f.write(repr(dict(bigramBeatPosTransitionProbabilities)))\n",
    "    \n",
    "    return bigramBeatPosTransitions, bigramBeatPosTransitionProbabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39091c-309f-4109-8c17-09033889373f",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a96b7-b8e8-4aa7-9c45-17838687dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9\n",
    "def beat_trigram_probability(midi_files):\n",
    "    x, y = 0, 0\n",
    "    CACHE_FILE = 'cache/trigram_beat_transitions.json'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "        x = ast.literal_eval(content)\n",
    "    \n",
    "    CACHE_FILE_2 = 'cache/trigram_beat_transition_probabilities.json'\n",
    "    if os.path.exists(CACHE_FILE_2):\n",
    "        with open(CACHE_FILE_2, 'r') as f:\n",
    "            content = f.read()\n",
    "        y = ast.literal_eval(content)\n",
    "        \n",
    "    if x and y:\n",
    "        return x, y\n",
    "    \n",
    "    trigramBeatTransitions = defaultdict(list)\n",
    "    trigramBeatTransitionProbabilities = defaultdict(list)\n",
    "    trigramTransitionCounts = defaultdict(lambda: defaultdict(int))\n",
    "    totalCounts = defaultdict(int)\n",
    "\n",
    "    for midi_file in midi_files:\n",
    "        beats = beat_extraction(midi_file)\n",
    "        # Need at least 3 beats to form a trigram\n",
    "        for i in range(1, len(beats)):\n",
    "            prev1_pos, prev1_len = beats[i - 1]\n",
    "            curr_pos, curr_len = beats[i]\n",
    "            key = (prev1_len, curr_pos)\n",
    "            trigramTransitionCounts[key][curr_len] += 1\n",
    "            totalCounts[key] += 1\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    for key in trigramTransitionCounts:\n",
    "        total = totalCounts[key]\n",
    "        next_lengths = list(trigramTransitionCounts[key].keys())\n",
    "        probs = [trigramTransitionCounts[key][n] / total for n in next_lengths]\n",
    "        trigramBeatTransitions[key] = next_lengths\n",
    "        trigramBeatTransitionProbabilities[key] = probs\n",
    "\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        f.write(repr(dict(trigramBeatTransitions)))\n",
    "    with open(CACHE_FILE_2, 'w') as f:\n",
    "        f.write(repr(dict(trigramBeatTransitionProbabilities)))\n",
    "\n",
    "    return trigramBeatTransitions, trigramBeatTransitionProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14edd6-b29f-4a71-9cfc-73f9f9d96727",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beats = [2,4,8]\n",
    "your_transition, your_probability = beat_trigram_probability(midi_files)\n",
    "yours = []\n",
    "for note in test_beats:\n",
    "    index = your_transition[(4, 0)].index(note)\n",
    "    yours.append(your_probability[(4, 0)][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0227c13-20d9-4fb6-b9b2-819283831afa",
   "metadata": {},
   "source": [
    "### Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868ae1b-eadf-4376-b6a5-4ae739e19dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_generate(length= 500, scale= \"C\", mode= \"Major\", keyword= \"Hopeful\"):\n",
    "    # Map scale name to root note MIDI number (assuming C4 = 60)\n",
    "    root_notes = {\n",
    "        \"C\": 60, \"C#\": 61,\n",
    "        \"Db\": 61, \"D\": 62, \"D#\": 63,\n",
    "        \"Eb\": 63, \"E\": 64,\n",
    "        \"F\": 65, \"F#\": 66,\n",
    "        \"Gb\": 66, \"G\": 67, \"G#\": 68,\n",
    "        \"Ab\": 68, \"A\": 69, \"A#\": 70,\n",
    "        \"Bb\": 70, \"B\": 71\n",
    "    }\n",
    "    # Major and minor scale intervals in semitones\n",
    "    scale_intervals = {\n",
    "        \"Major\": [0, 2, 4, 5, 7, 9, 11],\n",
    "        \"Minor\": [0, 2, 3, 5, 7, 8, 10]\n",
    "    }\n",
    "\n",
    "    # Get the root MIDI note number\n",
    "    root_note = root_notes.get(scale, 60)  # Default to C if not found\n",
    "    # Get the intervals for the specified mode\n",
    "    intervals = scale_intervals.get(mode, scale_intervals[\"Major\"])\n",
    "    # Calculate the scale notes (one octave)\n",
    "    scale_notes = {(root_note + interval) % 12 for interval in intervals}\n",
    "    \n",
    "    # Map for specific chords for Hopeful\n",
    "    chord_map = {\n",
    "    0: [33, 57, 60, 64],\n",
    "    1: [41, 57, 60, 65],\n",
    "    2: [36, 55, 60, 64],\n",
    "    3: [31, 55, 59, 62],\n",
    "    }\n",
    "    \n",
    "    # Shift chord notes by the scale root offset\n",
    "    def shift_chord(chord_notes, offset):\n",
    "        return [(note + offset) for note in chord_notes]\n",
    "\n",
    "    # Compute the offset\n",
    "    scale_root_mod = root_note % 12\n",
    "    # Apply shift to each chord\n",
    "    shifted_chords = {key: shift_chord(notes, scale_root_mod) for key, notes in chord_map.items()}\n",
    "\n",
    "    # List of chords for progression\n",
    "    chords = [shifted_chords[name] for name in [0, 1, 2, 3]]\n",
    "    allowed_notes = scale_notes\n",
    "\n",
    "    # sample notes\n",
    "    unigramProbabilities = note_unigram_probability(midi_files)\n",
    "    bigramTransitions, bigramTransitionProbabilities = note_bigram_probability(midi_files)\n",
    "    trigramTransitions, trigramTransitionProbabilities = note_trigram_probability(midi_files)\n",
    "\n",
    "    # Q10: Your code goes here ...\n",
    "    sampled_notes = []\n",
    "\n",
    "    # sample beats\n",
    "    sampled_beats = []\n",
    "\n",
    "    bigramBeatTransitions, bigramBeatTransitionProbabilities = beat_bigram_probability(midi_files)\n",
    "    bigramBeatPosTransitions, bigramBeatPosTransitionProbabilities = beat_pos_bigram_probability(midi_files)\n",
    "    trigramBeatTransitions, trigramBeatTransitionProbabilities = beat_trigram_probability(midi_files)\n",
    "\n",
    "    notes_list = list(unigramProbabilities.keys())\n",
    "    probs = list(unigramProbabilities.values())\n",
    "\n",
    "    # Function to filter notes based on scale\n",
    "    def filter_notes(note_candidates):\n",
    "        if allowed_notes is None:\n",
    "            return note_candidates\n",
    "        else:\n",
    "            return [n for n in note_candidates if n % 12 in allowed_notes]\n",
    "\n",
    "    # Initialize current note\n",
    "    current_note = np.random.choice(notes_list, p=probs)\n",
    "    current_note_candidates = filter_notes([current_note])\n",
    "    if current_note_candidates:\n",
    "        current_note = np.random.choice(current_note_candidates)\n",
    "    else:\n",
    "        # fallback if no notes in scale\n",
    "        current_note = np.random.choice(notes_list, p=probs)\n",
    "\n",
    "    prev_note1 = current_note  # for bigram\n",
    "    prev_note2 = None  # for trigram\n",
    "    prev_beat2 = None\n",
    "    prev_beat1 = None\n",
    "\n",
    "    # Initialize beat length\n",
    "    beat_length = np.random.choice(list(duration2length.values()))\n",
    "    current_beat_pos = 0\n",
    "\n",
    "    count_notes = 0\n",
    "    while count_notes < length:\n",
    "        # Generate next note using trigram model if possible\n",
    "        if prev_note2 is not None:\n",
    "            key = (prev_note2, prev_note1)\n",
    "            if key in trigramTransitions:\n",
    "                next_notes = trigramTransitions[key]\n",
    "                probs_trigram = trigramTransitionProbabilities[key]\n",
    "                # Filter next notes based on scale\n",
    "                filtered_next_notes = filter_notes(next_notes)\n",
    "                if filtered_next_notes:\n",
    "                    probs_filtered = [p for n, p in zip(next_notes, probs_trigram) if n in filtered_next_notes]\n",
    "                    next_note = np.random.choice(filtered_next_notes, p=np.array(probs_filtered)/sum(probs_filtered))\n",
    "                else:\n",
    "                    # fallback to unigram if no notes in scale\n",
    "                    next_note = np.random.choice(notes_list, p=list(unigramProbabilities.values()))\n",
    "            else:\n",
    "                # Fallback to unigram\n",
    "                next_note = np.random.choice(notes_list, p=list(unigramProbabilities.values()))\n",
    "        else:\n",
    "            # If only one previous note, fallback to bigram\n",
    "            next_note = np.random.choice(notes_list, p=list(unigramProbabilities.values()))\n",
    "            \n",
    "        # Generate beat length conditioned on previous two beats\n",
    "        if prev_beat2 is not None and prev_beat1 is not None:\n",
    "            key = (prev_beat2, prev_beat1)\n",
    "            if key in trigramBeatTransitions:\n",
    "                next_lengths = list(trigramBeatTransitions[key])\n",
    "                probs = trigramBeatTransitionProbabilities[key]\n",
    "                beat_length = np.random.choice(next_lengths, p=probs)\n",
    "            elif prev_beat1 in bigramBeatTransitions:\n",
    "                # fallback to bigram\n",
    "                next_lengths = list(bigramBeatTransitions[prev_beat1])\n",
    "                probs = bigramBeatTransitionProbabilities[prev_beat1]\n",
    "                beat_length = np.random.choice(next_lengths, p=probs)\n",
    "            else:\n",
    "                # fallback to random\n",
    "                beat_length = np.random.choice(list(duration2length.values()))\n",
    "        else:\n",
    "            # fallback\n",
    "            beat_length = np.random.choice(list(duration2length.values()))\n",
    "            \n",
    "        # Update previous two beats\n",
    "        prev_beat2 = prev_beat1\n",
    "        prev_beat1 = beat_length\n",
    "\n",
    "        # Append note info\n",
    "        sampled_notes.append(next_note)\n",
    "        sampled_beats.append(beat_length)\n",
    "\n",
    "        # Update previous notes for trigram model\n",
    "        prev_note2 = prev_note1\n",
    "        prev_note1 = next_note\n",
    "\n",
    "        # Update beat position\n",
    "        current_beat_pos += beat_length\n",
    "        if current_beat_pos >= 32:\n",
    "            # Reset to start of next bar\n",
    "            current_beat_pos = 0\n",
    "\n",
    "        count_notes += 1\n",
    "\n",
    "    # Ensure beat length is at least 1\n",
    "    sampled_beats = [i if i > 1 else i + 1 for i in sampled_beats]\n",
    "    sampled_beats = [i if (i == 1 or i % 2 == 0) else i-1 for i in sampled_beats]\n",
    "    sampled_beats = [i if (i < 4 or i % 4 == 0)  else i-2 for i in sampled_beats]\n",
    "    sampled_beats = [i if (i < 8 or i % 8 == 0)  else i-4 for i in sampled_beats]\n",
    "    sampled_beats = [i if (i < 16 or i % 16 == 0)  else i-2 for i in sampled_beats]\n",
    "\n",
    "    # Convert sampled notes and beats into MIDI\n",
    "    midi = MIDIFile(1)\n",
    "    track = 0\n",
    "    time = 0  # start at beat 0\n",
    "    midi.addTrackName(track, time, \"Generated Music\")\n",
    "    midi.addTempo(track, time, 120)\n",
    "\n",
    "    current_time = 0\n",
    "    total_time = 0\n",
    "    current_pos = 0  # beat position within the bar\n",
    "\n",
    "    for note, beat_len in zip(sampled_notes, sampled_beats):\n",
    "        # Convert beat beat_len to MIDI duration (divide by 8)\n",
    "        duration = beat_len / 8.0\n",
    "        # Add note: (track, channel, pitch, time, duration)\n",
    "        midi.addNote(track, 0, note, current_time, duration, 100)\n",
    "\n",
    "        # Increment time: move to next beat position\n",
    "        current_pos += beat_len\n",
    "        if current_pos >= 32:\n",
    "            # Reset position at the start of new bar\n",
    "            current_pos = 0\n",
    "            current_time += duration  # move to next bar\n",
    "        else:\n",
    "            current_time += duration\n",
    "\n",
    "    # Insert chords at the start, each as a whole note (duration=4 beats)\n",
    "    chord_time = 0\n",
    "    chord_duration = 4  # whole note\n",
    "    while (current_time - chord_time) // 16 >= 1 :  \n",
    "        for chord_notes in chords:\n",
    "            for note in chord_notes:\n",
    "                midi.addNote(track, 0, note, chord_time, chord_duration, 100)\n",
    "            # Move time forward by chord duration\n",
    "            chord_time += chord_duration\n",
    "            \n",
    "    # Save MIDI file\n",
    "    with open(f\"music/test/{scale}{mode}.mid\", \"wb\") as f:\n",
    "        midi.writeFile(f)\n",
    "\n",
    "    print(f\"Generated music in {scale} {mode} scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9ad2c-5703-434a-b4ed-1b901a8f7aeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b45732",
   "metadata": {
    "id": "77b45732"
   },
   "outputs": [],
   "source": [
    "def testQ1a():\n",
    "    yours = note_extraction(midi_files[0])\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88c9f7-9545-4e50-8ea8-83bfe06386a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ1a()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974f754",
   "metadata": {
    "id": "3974f754"
   },
   "outputs": [],
   "source": [
    "def testQ1b():\n",
    "    yours = note_frequency(midi_files)\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80dba7-aa0f-43a0-a024-ea4629a7f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ1b()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af9e67",
   "metadata": {
    "id": "68af9e67"
   },
   "outputs": [],
   "source": [
    "def testQ2():\n",
    "    yours = note_unigram_probability(midi_files)\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94045eff-5f0a-4808-9e2f-21c86aa7a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ2()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b101b8",
   "metadata": {
    "id": "b2b101b8"
   },
   "outputs": [],
   "source": [
    "def testQ3a():\n",
    "    your_transition, your_probability = note_bigram_probability(midi_files)\n",
    "    print(your_transition[74]) # Example\n",
    "    print(your_probability[74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666bdb0-d360-4c83-97f9-2de324dad463",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ3a()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2180b5",
   "metadata": {
    "id": "6d2180b5"
   },
   "outputs": [],
   "source": [
    "def testQ3b():\n",
    "    test_notes = [92, 35, 54] # some notes that have only one possible next note\n",
    "    yours = []\n",
    "    correct = []\n",
    "    for note in test_notes:\n",
    "        yours.append(sample_next_note(note))\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e6f32-1def-48b1-bcf4-f64d099da645",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ3b()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f201e8",
   "metadata": {
    "id": "93f201e8"
   },
   "outputs": [],
   "source": [
    "def testQ4():\n",
    "    test_file = midi_files[0]\n",
    "    yours = [note_bigram_perplexity(test_file)]\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677eb16-b45a-422e-9710-775b61cb681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ4()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dc0e3",
   "metadata": {
    "id": "c07dc0e3"
   },
   "outputs": [],
   "source": [
    "def testQ5a():\n",
    "    test_notes = [71,72,73]\n",
    "    your_transition, your_probability = note_trigram_probability(midi_files)\n",
    "    return your_transition, your_probability\n",
    "    # print(your_transition)\n",
    "    # print(your_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48365f-549e-4536-b51f-a20e5d8d8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "t1, t2 = testQ5a()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ac031",
   "metadata": {
    "id": "1b3ac031"
   },
   "outputs": [],
   "source": [
    "def testQ5b():\n",
    "    test_file = midi_files[0]\n",
    "    yours = [note_trigram_perplexity(test_file)]\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7720be-a530-4cdb-959d-9ab124c79991",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ5b()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20bc4",
   "metadata": {
    "id": "19f20bc4"
   },
   "outputs": [],
   "source": [
    "def testQ6():\n",
    "    test_files = midi_files[:5]\n",
    "    yours = []\n",
    "    for file in test_files:\n",
    "        beats = beat_extraction(file)\n",
    "        yours += [beat[0] for beat in beats]\n",
    "        yours += [beat[1] for beat in beats]\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c87bfb-b26d-4222-96ff-8efa235475eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ6()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5679384-fef5-459f-9e9d-82cee6b73a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CACHE_FILE_3 = 'cache/beat_extractions.json'\n",
    "# with open(CACHE_FILE_3, 'w') as f:\n",
    "#     json.dump(beat_extractions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61fd5f",
   "metadata": {
    "id": "5f61fd5f"
   },
   "outputs": [],
   "source": [
    "def testQ7():\n",
    "    test_beats = [2,4,8]\n",
    "    your_transition, your_probability = beat_bigram_probability(midi_files)\n",
    "    yours = []\n",
    "    correct = []\n",
    "    for note in test_beats:\n",
    "        index = your_transition[4].index(note)\n",
    "        yours.append(your_probability[4][index])\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c961a-a274-4e82-8c0b-63be0e306c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ7()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6191005",
   "metadata": {
    "id": "c6191005"
   },
   "outputs": [],
   "source": [
    "def testQ8a():\n",
    "    test_beats = [2,4,8]\n",
    "    your_transition, your_probability = beat_pos_bigram_probability(midi_files)\n",
    "    # print(your_transition, your_probability)\n",
    "    yours = []\n",
    "    for note in test_beats:\n",
    "        print(your_transition[0])\n",
    "        index = your_transition[0].index(note)\n",
    "        print(index)\n",
    "        yours.append(your_probability[0][index])\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef946ed0-f469-4d5e-868b-99b1f24b68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ8a()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e61d3",
   "metadata": {
    "id": "541e61d3"
   },
   "outputs": [],
   "source": [
    "def testQ8b():\n",
    "    test_file = midi_files[0]\n",
    "    yours = list(beat_bigram_perplexity(test_file))\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a5ba-8470-4f9b-bea5-5970430d2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ8b()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19641f59",
   "metadata": {
    "id": "19641f59"
   },
   "outputs": [],
   "source": [
    "def testQ9a():\n",
    "    test_beats = [2,4,8]\n",
    "    your_transition, your_probability = beat_trigram_probability(midi_files)\n",
    "    yours = []\n",
    "    for note in test_beats:\n",
    "        index = your_transition[(4, 0)].index(note)\n",
    "        yours.append(your_probability[(4, 0)][index])\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc31621-4497-4e94-8d2a-8d5ab4c5afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ9a()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6bae4",
   "metadata": {
    "id": "3dc6bae4"
   },
   "outputs": [],
   "source": [
    "def testQ9b():\n",
    "    test_file = midi_files[0]\n",
    "    yours = [beat_trigram_perplexity(test_file)]\n",
    "\n",
    "    print(yours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ea071-940f-4a55-a34b-8b46cefc6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ9b()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39cada-7582-4c77-a532-a40da9db1ab7",
   "metadata": {},
   "source": [
    "## Generate Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338b9e8",
   "metadata": {
    "id": "d338b9e8"
   },
   "outputs": [],
   "source": [
    "# def testQ10():\n",
    "#     for mode in [\"Major\", \"Minor\"]:\n",
    "#         for scale in [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]:\n",
    "#             finished = 0\n",
    "#             while not finished:\n",
    "#                 try:\n",
    "#                     music_generate(500, scale, mode)\n",
    "#                     finished = 1\n",
    "#                 except Exception as e:\n",
    "#                     print(\"Trying again\")\n",
    "def testQ10():\n",
    "    music_generate(500, \"F\", \"Major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb83ec3-cae3-43ce-bcdf-bba6a37edac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1_start = time.time()\n",
    "testQ10()\n",
    "t1_end = time.time()\n",
    "print(f\"Test finished in {round(t1_end - t1_start, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb40333-f832-4aa4-8797-a5cec424ad24",
   "metadata": {},
   "source": [
    "# Analysis and Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54034e3-18f9-418b-a79c-a554e51646d9",
   "metadata": {},
   "source": [
    "Based on the audio outputs from these models, 2-dimensional Variational Autoencoders have resulted in the most audibly appealing MIDI compositions. This model is also the fastest to generate music (given that caching is not used), making it the most effective model this project has developed. While the model has only been trained to generate symbolic musical compositions, VAEs are great for use in continuous audio generation, such as .wav formats. The implementation is similar in style to our models: take as input a melSpectrogram or audio file, encode into a set of parameters, decode back to a melSpectrogram and back to an audio format of choice. Perhaps in the near future this project will be updated to include samples of generated music in this manner.\n",
    "\n",
    "Additional neural networks and deep learning models to try for symbolic music generation include Generative Adversarial Networks (GANs) and Long Short-Term Memory (LSTMs).\n",
    "\n",
    "Finally, we should discuss how our datasets have been used in the past. With the increased calls for artist protection against AI copyright infringement, there needed to be useful copyright-free training data to train models while being up to date with modern music trends. \n",
    "\n",
    "PDMX has been used to train music generation models and for MIDI conversion applications. While VAEs and Markov Chains are useful for symbolic generation, others have opted to use LSTMs, CNNs, GANs, and HMMs. Results are difficult to compare from a mathematical perspective, but our music generation differs from others in that either chords or single notes were prevalent in most of the music; a combination of both was rarely ever observed.\n",
    "\n",
    "MAESTRO is a dataset composed of over 200 hours of piano compositions. This data comes in both MIDI and wav formats. This dataset has been used for many ML algorithms related to remote piano composition judging.\n",
    "\n",
    "**While this method of generating music is no longer the state of the art, the knowledge acquired from this project has vastly improved my understanding of machine learning concepts, and the implementation of Convolutional Neural Networks shows that I can effectively apply these skills to real-world situations.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
